# 👨‍🎓 About Me
**Ioannis Papadopoulos**  
MSc in Data Science  
Athens University of Economics and Business  
Current Role: Software Engineer, Costamare  
📧 johhnyp7.gp@gmail.com  
🔗 https://www.linkedin.com/in/giannis-papadopoulos2001/

---

## 📂 MSc Data Science Portfolio  
Ioannis Papadopoulos – Athens University of Economics and Business

Welcome to my academic portfolio developed throughout the MSc in Data Science program at AUEB. This repository compiles key course deliverables, projects, and research work across machine learning, SQL, deep learning, optimization, and data communication. It reflects both theoretical knowledge and practical problem-solving applied to real-world data challenges.

---

## 📚 Program Overview

The MSc in Data Science at AUEB offers a comprehensive curriculum in statistics, machine learning, optimization, data engineering, and ethical AI practices. The coursework integrates hands-on experience with state-of-the-art tools, frameworks, and real datasets, fostering both depth and breadth in data science expertise.

Each entry corresponds to a course and includes assignments, code, final reports, and practical applications.

---

## 🧭 Course Structure by Trimester

### 📘 Trimester 1

#### 📌 Introduction to Data Management and Engineering  
**Focus**: Relational database systems and performance optimization

- ER modeling, normalization, schema design  
- SQL scripting with PostgreSQL  
- Views, constraints, indexing, and query plans  
- Optimization using EXPLAIN and indexing strategies  


#### 📌 Machine Learning and Computational Statistics  
**Focus**: Core machine learning models and statistical methods

- Density estimation (parametric/non-parametric)  
- Linear/logistic regression, Lasso, Ridge  
- SVMs, Naive Bayes, PCA  
- Final project: Hyperspectral image classification (Salinas dataset)  
- Tools: scikit-learn, cvxpy, matplotlib, numpy  


#### 📌 Practical Data Science  
**Focus**: Applied machine learning with real-world datasets

- Data preprocessing and feature engineering  
- NLP with BERT/RoBERTa for multi-label classification  
- Image clustering using deep learning embeddings  
- Participation in SemEval 2025: Food Hazard Detection Challenge  
- Tools: HuggingFace Transformers, LightGBM, ResNet101  


#### 📌 Probability and Statistics for Data Analysis  
**Focus**: Statistical inference and probabilistic modeling in R

- Bayes’ theorem, probability distributions, Poisson process  
- Hypothesis testing, confidence intervals, ANOVA  
- Regression modeling and interaction terms  
- Tools: R, ggplot2, stats, car, lm  


---

### 📘 Trimester 2

#### 📌 Text Analytics  
**Focus**: Natural language processing using traditional and neural approaches

- N-grams, perplexity, Levenshtein distance  
- Deep NLP models: MLPs, CNNs, RNNs with attention  
- Transformer fine-tuning (BERT, RoBERTa) for classification  
- Evaluation metrics: Accuracy, F1, AUC  
- Tools: PyTorch, HuggingFace, TensorFlow, Google Colab  


#### 📌 Numerical Optimization and Large-Scale Linear Algebra  
**Focus**: Mathematical optimization and matrix computations

- Constrained optimization, SVD, PageRank  
- Iterative solvers: Gauss-Seidel, Power Method  
- Kernel methods and digit recognition  
- Tools: numpy, scipy.sparse, custom solvers  


#### 📌 Large Scale Data Management  
**Focus**: Distributed systems and real-time data pipelines

- Hadoop MapReduce (Java, Maven)  
- Streaming pipelines with Apache Kafka, PySpark, and Cassandra  
- Structured streaming, stateful transformations  
- Topics: HDFS, job scheduling, streaming joins


#### 📌 Legal and Ethical Issues in Data Science  
**Focus**: Data privacy, fairness, and responsible AI

- GDPR, data governance, and personal data rights  
- Algorithmic bias, discrimination, and accountability  
- Case studies: Cambridge Analytica, social scoring  
- Policy design for ethical AI systems  

#### 📌 Statistics for Big Data  
**Focus**: Statistical methods for high-dimensional and complex datasets in the era of big data

- Challenges with traditional methods on large-scale datasets  
- Regularization techniques for variable selection (Lasso, Ridge, Elastic Net)  
- Multiplicity problems: False Discovery Rate, Family-wise Error Rate  
- Genetic data analysis: SAM, Supervised PCA, Genome-wide association methods  
- Statistical network analysis: bipartite networks, latent space models, random subgraphs  
- Practical implementation using R and specialized statistical packages  

---

### 📘 Trimester 3


#### 📌 Introduction to Quantitative Finance and Financial Risk Management  
**Focus**: Financial instruments, asset modeling, and quantitative risk analysis

- Fixed income securities: interest rate curves, duration, convexity, valuation  
- Derivatives: forwards, futures, swaps, and options under the Black–Scholes framework  
- Risk metrics: Value at Risk (VaR), Expected Shortfall, Potential Future Exposure  
- VaR methodologies: historical simulation, parametric models, Monte Carlo simulation  
- Credit risk modeling: Merton model, CreditMetrics, Basel II capital requirements  
- Hands-on Monte Carlo simulations and time series analysis of real-world market data  



#### 📌 Data Visualization and Communication  
**Focus**: Visual storytelling through static, animated, and interactive dashboards

- **Project 1**: Greece in the Global Classroom – PISA 2018  
  - Educational performance analysis by subject, gender, SES, and investment  
  - Tools: ggplot2, dplyr, gganimate, Tableau  
- **Project 2**: France on the Road – Road Accidents 2009–2012  
  - Temporal and geographic accident trends  
  - Integration of static and animated visualizations  
  - Tools: ggplot2, gganimate, Tableau  

**Outcomes**: Policy recommendations through storytelling dashboards  

---

## 📈 Closing Remarks

This portfolio showcases the work completed during my MSc in Data Science program at [University], highlighting my practical application of data science methodologies across various projects. These projects reflect my ability to integrate cross-disciplinary knowledge and utilize advanced tools to tackle complex data-related challenges.

While the majority of the work is my own, I have utilized tools such as Large Language Models (LLMs) for assistance in generating ideas, structuring content, and refining certain sections. Proper attribution has been given to all external sources and tools used, ensuring compliance with academic integrity policies.

This work is intended for personal academic and professional use only and is not to be used or reproduced by others without proper attribution or explicit permission.
